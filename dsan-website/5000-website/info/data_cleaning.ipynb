{
  "cells": [
    {
      "cell_type": "raw",
      "id": "f5025443",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data Cleaning\"\n",
        "backgroundcolor: white\n",
        "mainfont: Georgia\n",
        "theme: morph\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083f15c2",
      "metadata": {},
      "source": [
        "The cleaning of both textual and quantitative as no simple solution as each dataset has to be treated separately. Hence, the data will be tackled separately.\n",
        "\n",
        "### Quantitative Data\n",
        "\n",
        "For the quantitative data, the way you clean is completely dependent on the nature of the text. Sometimes, cleaning can take a very small period of time and sometimes, it can get a bit too much. As a data scientist, caution has to be taken to ensure each dataset is handled separately. Using the Python and R API, since we have pulled in the US data, this is show it looks.\n",
        "\n",
        "###### Python API\n",
        "\n",
        "For the Python API, where the data had been retrieved from FREDAPI, there was not a lot of cleaning that was required. After merging the data, there was one missing value that was imputed using the Mean Value Imputation. While the MVA is not a recommended method for dealing with missing values, and usually for economic data, missing values are not imputed for the data as it combats the purpose of analysis. If our main gist is to analyse the data and their trends across the nations, the imputing could give us biased estimates. Yet, since the amount of missing values are really less, MVA seems like a better option.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba75590f",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from fredapi import Fred\n",
        "import pandas as pd\n",
        "\n",
        "fred = Fred(api_key='10fbe66f8f62ad7f44097cca867bf01f')\n",
        "\n",
        "series_id_1 = 'GDP' # for GDP US\n",
        "series_id_2='GDPC1' # for Real Gross GDP\n",
        "series_id_3='W207RC1Q156SBEA' # Adjusted Saving (% of GNI)\n",
        "series_id_4='USAB6BLTT02STSAQ' # Current account balance (% of GDP)\n",
        "series_id_5='A019RE1Q156NBEA' # Export of Goods and Services as percentage of GDP\n",
        "series_id_6='MKTGNIUSA646NWDB' # GNI\n",
        "\n",
        "# Set the start and end dates where I need data from 1990 to 2023\n",
        "start_date = '1973'\n",
        "end_date = '2023'\n",
        "\n",
        "# Fetch the data\n",
        "gdp_u = fred.get_series(series_id_1, start_date, end_date)\n",
        "realgdp_u = fred.get_series(series_id_2, start_date, end_date)\n",
        "adjsavings_u = fred.get_series(series_id_3, start_date, end_date)\n",
        "currentaccbalance_u = fred.get_series(series_id_4, start_date, end_date)\n",
        "exportsofgoods_u = fred.get_series(series_id_5, start_date, end_date)\n",
        "gni_u= fred.get_series(series_id_6, start_date, end_date)\n",
        "\n",
        "# for GDP\n",
        "df1= gdp_u.to_frame(name=('GDP'))\n",
        "annual_gdp_u = df1.resample('A').mean()\n",
        "annual_gdp_u['Year'] = annual_gdp_u.index.year\n",
        "annual_gdp_u=annual_gdp_u[['Year', 'GDP']]\n",
        "annual_gdp_u.reset_index(drop=True, inplace=True)\n",
        "print(annual_gdp_u.head(5))\n",
        "\n",
        "# Real GDP\n",
        "df2= realgdp_u.to_frame(name=('Real GDP'))\n",
        "annual_realgdp_u = df2.resample('A').mean()\n",
        "annual_realgdp_u['Year'] = annual_realgdp_u.index.year\n",
        "annual_realgdp_u.reset_index(drop=True, inplace=True)\n",
        "annual_realgdp_u.head(16)\n",
        "\n",
        "# Adjusted Savings\n",
        "df3= adjsavings_u.to_frame(name=('Adjusted Savings'))\n",
        "annual_adjsavings_u = df3.resample('A').mean()\n",
        "annual_adjsavings_u['Year'] = annual_adjsavings_u.index.year\n",
        "annual_adjsavings_u.reset_index(drop=True, inplace=True)\n",
        "annual_adjsavings_u.head(2)\n",
        "\n",
        "# Current Account Balance % of GDP\n",
        "df4= currentaccbalance_u.to_frame(name=('Current Account Balance'))\n",
        "annual_currentaccbalance_u = df4.resample('A').mean()\n",
        "annual_currentaccbalance_u['Year'] = annual_currentaccbalance_u.index.year\n",
        "annual_currentaccbalance_u.reset_index(drop=True, inplace=True)\n",
        "annual_currentaccbalance_u.head(2)\n",
        "\n",
        "# Current Account Balance % of GDP\n",
        "df5= exportsofgoods_u.to_frame(name=('Exports of Goods and Services'))\n",
        "annual_exportsofgoods_u = df5.resample('A').mean()\n",
        "annual_exportsofgoods_u['Year'] = annual_exportsofgoods_u.index.year\n",
        "annual_exportsofgoods_u.reset_index(drop=True, inplace=True)\n",
        "annual_exportsofgoods_u.head(2)\n",
        "\n",
        "# GNI\n",
        "df6= gni_u.to_frame(name=('GNI'))\n",
        "annual_gni_u = df6.resample('A').mean()\n",
        "annual_gni_u['Year'] = annual_gni_u.index.year\n",
        "annual_gni_u.reset_index(drop=True, inplace=True)\n",
        "annual_gni_u.head(10)\n",
        "\n",
        "merged_df= pd.merge(annual_gdp_u, annual_realgdp_u, how= 'outer', on= 'Year' )\n",
        "merged_df= pd.merge(merged_df, annual_adjsavings_u, how= 'outer', on= 'Year')\n",
        "merged_df= pd.merge(merged_df, annual_currentaccbalance_u, how= 'outer', on= 'Year' )\n",
        "merged_df= pd.merge(merged_df, annual_exportsofgoods_u, how= 'outer', on= 'Year' )\n",
        "merged_df= pd.merge(merged_df, annual_gni_u, how= 'outer', on= 'Year' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7823f8",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "merged_df= pd.merge(annual_gdp_u, annual_realgdp_u, how= 'outer', on= 'Year' )\n",
        "merged_df= pd.merge(merged_df, annual_adjsavings_u, how= 'outer', on= 'Year')\n",
        "merged_df= pd.merge(merged_df, annual_currentaccbalance_u, how= 'outer', on= 'Year' )\n",
        "merged_df= pd.merge(merged_df, annual_exportsofgoods_u, how= 'outer', on= 'Year' )\n",
        "merged_df= pd.merge(merged_df, annual_gni_u, how= 'outer', on= 'Year' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c01c299",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "merged_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f563ba84",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "merged_df['GNI'] = merged_df['GNI'].fillna(merged_df['GNI'].mean())\n",
        "\n",
        "merged_df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df0fec4",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "merged_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a5c356d",
      "metadata": {},
      "source": [
        "###### R API\n",
        "\n",
        "For the R API, a similar aspect is seen. When a set of macroeconomic and fiscal variables have been pulled from python, few were also pulled from R using the 'wbstats' API.\n",
        "\n",
        "Similar to the Python API, the data retrieved from here also didn't have a lot of data cleaning steps.\n",
        "\n",
        "<img src=\"/images/wbstats.2.png\" width= \"500\" height=\"300\" />\n",
        "\n",
        "Here you can see the amount of missing values wherein, we similarly cleaned it through missing value imputation through mean.\n",
        "\n",
        "\n",
        "```{markdown}\n",
        "\n",
        "# Using mean value imputation \n",
        "for (x in names(merged_df)) {\n",
        "  mean <- mean(merged_df[[x], na.rm = TRUE)  # Calculate column mean ignoring NA\n",
        "  merged_df[[x]][is.na(merged_df[[x]])] <- mean    # Replace NA with column mean\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Here is no missing of the data\n",
        "\n",
        "<img src=\"/images/wbstats.3.png\" width= \"500\" height=\"300\" />\n",
        "\n",
        "The final representation of the data\n",
        "\n",
        "<img src=\"/images/wbstats.4.png\" width= \"500\" height=\"300\" />\n",
        "\n",
        "\n",
        "### Textual Data\n",
        "\n",
        "##### Python API\n",
        "\n",
        "For the textual data, the python wrapper of PRAW has been used. This is how the output looks\n",
        "\n",
        "<img src=\"/images/reddit2.png\" width= \"500\" height=\"300\" />\n",
        "\n",
        "This was used to clean and vectorise the data.\n",
        "\n",
        "\n",
        "```{markdown}\n",
        "\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "def clean_string(text):\n",
        "    # lowercase the text\n",
        "    text = text.lower()\n",
        "    \n",
        "    # replace with regular quotations, added this since there were few texts that weren't cleaned\n",
        "    text = text.replace('â€™', \"'\")\n",
        "    \n",
        "    # remove emojis, saw a list of them online so just added them.\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                           u\"\\U0001F004-\\U0001F0CF\"  # Miscellaneous Symbols and Pictographs\n",
        "                           u\"\\U0001F0D0-\\U0001F0FF\"  # Emoticons\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # Regional Indicator Symbols\n",
        "                           u\"\\U0001F200-\\U0001F251\"  # Enclosed Ideographic Supplement\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)  # Remove emojis\n",
        "    \n",
        "    # line breaks removal\n",
        "    text = text.replace('\\n', ' ')\n",
        "    \n",
        "    #  punctuation removal\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    \n",
        "    #  tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "    \n",
        "    # remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    \n",
        "    # rejoin the words into a cleaned string\n",
        "    cleaned_text = ' '.join(words)\n",
        "    \n",
        "    return cleaned_text\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Then apply the clean string in addition. \n",
        "\n",
        "\n",
        "```{markdown}\n",
        "\n",
        "df['cleaned_tweets'] = df['body'].apply(clean_string)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "<img src=\"/images/reddit3.png\" width= \"500\" height=\"300\" />\n",
        "\n",
        "We then vectorise the data using the 'CountVectoriser' from sklearn\n",
        "\n",
        "\n",
        "```{markdown}\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "value = [str(element) for element in df2]\n",
        "vectorizer=CountVectorizer()\n",
        "Xs  =  vectorizer.fit_transform(value)   \n",
        "print(type(Xs))\n",
        "print(\"vocabulary = \",vectorizer.vocabulary_)   \n",
        "print(\"stop words =\", vectorizer.stop_words)\n",
        "col_names=vectorizer.get_feature_names_out()\n",
        "print(\"col_names=\",col_names)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "This is how the actual output looks like\n",
        "\n",
        "<img src=\"/images/reddit4.png\" width= \"500\" height=\"300\" />\n",
        "\n",
        "In addition to that, since we are also looking for perform Naive Bayes next, which is often used for categorical data, we use the SIA, or the Sentiment Intensity Analyser from the NLTK that automatically calculates the polarity scores on basis of the tweets. This is required as the data I retrieved didn't have any labels to it. This seemed like the best option.\n",
        "\n",
        "\n",
        "```{markdown}\n",
        "\n",
        "sia = SIA()\n",
        "results = []\n",
        "\n",
        "for tweet in df['cleaned_tweets']:  \n",
        "    polarity = sia.polarity_scores(tweet)\n",
        "    polarity['cleaned_tweets'] = tweet  cores\n",
        "    results.append(polarity)\n",
        "\n",
        "\n",
        "sentiment_df = pd.DataFrame(results)\n",
        "pprint(sentiment_df.head())\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "<img src=\"/images/reddit5.png\" width= \"500\" height=\"300\" />\n",
        "\n",
        "### Record Data\n",
        "\n",
        "For the record data, using the world bank data bank, the data was retrieved. There is a lot of cleaning that was supposed to be done. The entire data as seen under data_gathering section was transposed and had 152 columns. I had to individually transpose. Below, a similar method was imposed to clean all the 5 individual datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d3307e",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#pip install xlrd\n",
        "df=pd.read_excel('../data/excel_files/india.xlsx')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e73759ad",
      "metadata": {},
      "source": [
        "I had to transpose the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55dded5d",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df = df.T\n",
        "df.columns = df.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6beaff",
      "metadata": {},
      "source": [
        "The tranposed data is shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212c894a",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e25f973",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df = df.iloc[4:]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad95d2b1",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df = df.iloc[1:]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e03388c",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "892eefa0",
      "metadata": {},
      "source": [
        "In the dataset, there was a problem of the dataframe having row indexes and column indexes and hence, I worked on it drop the index and create a yearly column from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519f3efd",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# creating a new index rather than treating it\n",
        "new_index = range(1979, 2022)\n",
        "\n",
        "# Reset the index with the new range\n",
        "df = df.reset_index(drop=True)\n",
        "df.index = new_index\n",
        "df.index.name = \"year\"\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270c7100",
      "metadata": {},
      "source": [
        "I renamed all the columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf49f21",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "cols= ['adj_NNI_g', 'adj_NNI_c', 'adj_savings_fix_cap_GNI', 'adj_savings_edu_GNI', 'adj_NNS_GNI',\n",
        "                    'curr_acc_bal', 'ex_imp_gdp', 'ex_imp_growth', 'ex_debt_shocks', 'final_consump_exp_gdp',\n",
        "                    'final_consump_exp_growth', 'fdi_net_inflows', 'fdi_net_outflows', 'gdp_growth', 'gdp_per_capita_growth',\n",
        "                    'gener_govt_fin_consump_exp', 'gni_growth', 'goods_exp_bop','goods_imp_bop', 'imports_goods_services_gdp', 'imports_goods_services_growth','services_gdp', 'services_growth', \n",
        "                       'short_term_debt_tot_reserves','trade_gdp', 'cpi', 'inflation','real_interest_rate','lending_interest_rate','life_exp_birth','expense_gdp','interest_payments', 'military expenditure']\n",
        "\n",
        "df.columns = cols\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff4af68d",
      "metadata": {},
      "source": [
        "Used mean value imputation for missing value imputation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98634a12",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "value_to_replace = \"..\"\n",
        "for col in df.columns:\n",
        "    df[col] = df[col].replace(value_to_replace, np.nan)\n",
        "\n",
        "missing= df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd1763d9",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "missing_vals = pd.DataFrame({'Missing Values': missing.index, 'Missing Values Sum': missing.values})\n",
        "missing_vals = missing_vals.sort_values(by='Missing Values Sum', ascending=False)\n",
        "\n",
        "print(missing_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f2e164",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "for cols in df.columns:\n",
        "    mean = df[cols].mean()\n",
        "    df[cols] = df[cols].fillna(mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1619deb",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f77813",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51d182d",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df['year'] = df.index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634f640f",
      "metadata": {},
      "source": [
        "Next for the purpose of Naive Bayes, binning has to be done for which I looked at the maximum and minimum values of the gdp growth and binned them accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21888472",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df[\"gdp_growth\"].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b36b58",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df[\"gdp_growth\"].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba143a4",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "bins = [0, 2, 5, 7, float('inf')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbcd3f9e",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "labels = ['Low Growth', 'Moderate Growth', 'High Growth', 'Very High Growth']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e86191b",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df['labels'] = pd.cut(df['gdp_growth'], bins=bins, labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4020a44",
      "metadata": {},
      "source": [
        "This is the final cleaned df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5859643",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
